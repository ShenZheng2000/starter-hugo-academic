---
title: Study of Graph Theory, Distributed Average Consensus Algorithm and Centralized Algorithm
summary: An survey for basic graphs, consensus algorithms, distributed average schemes, and centralized algorithms.
tags:
- Machine Learning
- Graph Theory
date: "2020-12-01T00:00:00Z"
# tags就是project会归类到哪里，可以有多个tag

# Optional external URL for project (replaces project detail page).
external_link: ""

# caption就相当于reference，这个图片是哪里来的，会显示在图像右下角
image:
  caption: ''
  focal_point: Smart

# 不要用url_X，直接加icon，如果paper可以加笔啥的，参考我的
links:
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
# slide可以暂时不要

---
**Abstract**

The Asian giant hornet (AGH) appeared in Washington State appears to have a potential danger of bioinvasion. Washington State has collected public photos and videos of detected insects for verification and further investigation. In this paper, we analyze AGH using data analysis, statistics, discrete mathematics, and deep learning techniques to process the data to control AGH spreading. First, we visualize the geographical distribution of insects in Washington State. Then we investigate insect populations to varying months of the year and different days of a month. Third, we employ wavelet analysis to examine the periodic spread of AGH. Fourth, we apply ordinary differential equations to examine AGH numbers at the different natural growth rate and reaction speed and output the potential propagation coefficient. Next, we leverage cellular automaton combined with the potential propagation coefficient to simulate the geographical spread under changing potential propagation. To update the model, we use delayed differential equations to simulate human intervention. We use the time difference between detection time and submission time to determine the unit of time to delay time. After that, we construct a lightweight CNN called SqueezeNet and assess its classification performance. We then relate several non-reference image quality metrics, including NIQE, image gradient, entropy, contrast, and TOPSIS to judge the cause of misclassification. Furthermore, we build a Random Forest classifier to identify positive and negative samples based on image qualities only. We also display the feature importance and conduct an error analysis. Besides, we present sensitivity analysis to verify the robustness of our models. Finally, we show the strengths and weaknesses of our model and derives the conclusions.


